{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7SuoZIQNNKLy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_GPU\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                           precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                           matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score)\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Load and setup the Llama model\n",
        "print(\"Loading Llama 3.1 8B model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = 8192,\n",
        "    load_in_4bit = True,\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")\n",
        "\n",
        "# Setup chat template for Llama\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Model loaded successfully! Using Llama 3.1 8B for multiple choice questions.\")\n",
        "\n",
        "# Load your dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('/content/hard_120_samples.csv')\n",
        "print(f\"Dataset loaded: {len(df)} rows\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"True label distribution:\\n{df['true_label'].value_counts()}\")\n",
        "\n",
        "def predict_multiple_choice(row):\n",
        "    \"\"\"\n",
        "    Predict the correct answer for a multiple choice question using Llama\n",
        "    Returns tuple: (prediction, raw_response)\n",
        "    \"\"\"\n",
        "    sentence = row['sentence']\n",
        "    option_A = row['option_A']\n",
        "    option_B = row['option_B']\n",
        "    option_C = row['option_C']\n",
        "    option_D = row['option_D']\n",
        "\n",
        "    # Create a comprehensive prompt optimized for Llama\n",
        "    prompt = f\"\"\"You are an expert annotator for multiple-choice classification.\n",
        "\n",
        "Given the sentence below, select the most appropriate answer.\n",
        "\n",
        "Sentence: \"{sentence}\"\n",
        "\n",
        "Options:\n",
        "• A: {option_A}\n",
        "• B: {option_B}\n",
        "• C: {option_C}\n",
        "• D: {option_D}\n",
        "\n",
        "**Respond with only a single letter (A, B, C, or D) and nothing else.**\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"from\": \"human\", \"value\": prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate response with parameters optimized for Llama\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=10,       # Very limited tokens for single letter answer\n",
        "            use_cache=True,\n",
        "            do_sample=False,         # Llama works better with sampling\n",
        "            temperature=0.01,        # Low temperature for consistency\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    raw_response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Clean and extract prediction\n",
        "    response_clean = raw_response.replace('\\n', ' ').strip().upper()\n",
        "\n",
        "    # Look for the first occurrence of A, B, C, or D\n",
        "    prediction = None\n",
        "    for char in response_clean:\n",
        "        if char in ['A', 'B', 'C', 'D']:\n",
        "            prediction = char\n",
        "            break\n",
        "\n",
        "    # If no clear answer found, try alternative extraction\n",
        "    if prediction is None:\n",
        "        # Check if response starts with a letter\n",
        "        if response_clean.startswith('A'):\n",
        "            prediction = 'A'\n",
        "        elif response_clean.startswith('B'):\n",
        "            prediction = 'B'\n",
        "        elif response_clean.startswith('C'):\n",
        "            prediction = 'C'\n",
        "        elif response_clean.startswith('D'):\n",
        "            prediction = 'D'\n",
        "        # Check if response contains only one of the letters\n",
        "        elif 'A' in response_clean and all(x not in response_clean for x in ['B', 'C', 'D']):\n",
        "            prediction = 'A'\n",
        "        elif 'B' in response_clean and all(x not in response_clean for x in ['A', 'C', 'D']):\n",
        "            prediction = 'B'\n",
        "        elif 'C' in response_clean and all(x not in response_clean for x in ['A', 'B', 'D']):\n",
        "            prediction = 'C'\n",
        "        elif 'D' in response_clean and all(x not in response_clean for x in ['A', 'B', 'C']):\n",
        "            prediction = 'D'\n",
        "        else:\n",
        "            # Default to A if completely unclear\n",
        "            prediction = 'A'\n",
        "\n",
        "    return prediction, raw_response\n",
        "\n",
        "def evaluate_dataset(df, sample_size=None):\n",
        "    \"\"\"\n",
        "    Evaluate the entire dataset or a sample\n",
        "    Returns predictions, true_labels, and raw_responses\n",
        "    \"\"\"\n",
        "    if sample_size:\n",
        "        df_eval = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "        print(f\"Evaluating sample of {sample_size} questions...\")\n",
        "    else:\n",
        "        df_eval = df.copy()\n",
        "        print(f\"Evaluating all {len(df_eval)} questions...\")\n",
        "\n",
        "    predictions = []\n",
        "    raw_responses = []\n",
        "    true_labels = df_eval['true_label'].tolist()\n",
        "\n",
        "    for idx, row in df_eval.iterrows():\n",
        "        print(f\"\\nEvaluating {idx+1}/{len(df_eval)}: {row['sentence'][:50]}...\")\n",
        "        print(f\"Domain: {row['domain']}\")\n",
        "\n",
        "        try:\n",
        "            prediction, raw_response = predict_multiple_choice(row)\n",
        "            predictions.append(prediction)\n",
        "            raw_responses.append(raw_response)\n",
        "            print(f\"Raw Response: {raw_response}\")\n",
        "            print(f\"Prediction: {prediction}, True: {row['true_label']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question {idx+1}: {e}\")\n",
        "            predictions.append('A')  # Default to A on error\n",
        "            raw_responses.append(f\"ERROR: {str(e)}\")\n",
        "\n",
        "    return predictions, true_labels, raw_responses, df_eval\n",
        "\n",
        "def calculate_comprehensive_metrics(predictions, true_labels):\n",
        "    \"\"\"\n",
        "    Calculate and display comprehensive evaluation metrics for multiple choice\n",
        "    \"\"\"\n",
        "    # Calculate accuracy\n",
        "    correct = sum(1 for p, t in zip(predictions, true_labels) if p == t)\n",
        "    accuracy = correct / len(predictions)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    labels = ['A', 'B', 'C', 'D']\n",
        "    cm = np.zeros((4, 4), dtype=int)\n",
        "\n",
        "    for true, pred in zip(true_labels, predictions):\n",
        "        true_idx = labels.index(true)\n",
        "        pred_idx = labels.index(pred)\n",
        "        cm[true_idx][pred_idx] += 1\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    class_accuracies = {}\n",
        "    class_predictions = {}\n",
        "    class_totals = {}\n",
        "\n",
        "    for label in labels:\n",
        "        class_totals[label] = true_labels.count(label)\n",
        "        class_predictions[label] = sum(1 for t, p in zip(true_labels, predictions) if t == label and t == p)\n",
        "        if class_totals[label] > 0:\n",
        "            class_accuracies[label] = class_predictions[label] / class_totals[label]\n",
        "        else:\n",
        "            class_accuracies[label] = 0.0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"COMPREHENSIVE EVALUATION METRICS - LLAMA 3.1\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\"\\n--- Overall Metrics ---\")\n",
        "    print(f\"Total Questions: {len(predictions)}\")\n",
        "    print(f\"Correct Predictions: {correct}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\n--- Per-Class Accuracy ---\")\n",
        "    for label in labels:\n",
        "        print(f\"Option {label}: {class_accuracies[label]:.4f} ({class_predictions[label]}/{class_totals[label]})\")\n",
        "\n",
        "    print(f\"\\n--- Confusion Matrix ---\")\n",
        "    print(\"True\\\\Pred\\tA\\tB\\tC\\tD\")\n",
        "    for i, label in enumerate(labels):\n",
        "        row_str = f\"{label}\\t\\t\"\n",
        "        for j in range(4):\n",
        "            row_str += f\"{cm[i][j]}\\t\"\n",
        "        print(row_str)\n",
        "\n",
        "    # Create confusion matrix visualization\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels,\n",
        "                yticklabels=labels)\n",
        "    plt.title('Confusion Matrix - Llama 3.1 8B Multiple Choice')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('llama_confusion_matrix_mcq.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate prediction distribution\n",
        "    pred_distribution = {label: predictions.count(label) for label in labels}\n",
        "    true_distribution = {label: true_labels.count(label) for label in labels}\n",
        "\n",
        "    print(f\"\\n--- Label Distribution ---\")\n",
        "    print(\"True Label Distribution:\")\n",
        "    for label in labels:\n",
        "        count = true_distribution[label]\n",
        "        print(f\"  Option {label}: {count} ({count/len(true_labels)*100:.2f}%)\")\n",
        "\n",
        "    print(\"\\nPredicted Label Distribution:\")\n",
        "    for label in labels:\n",
        "        count = pred_distribution[label]\n",
        "        print(f\"  Option {label}: {count} ({count/len(predictions)*100:.2f}%)\")\n",
        "\n",
        "    # Return metrics dictionary\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'correct_predictions': correct,\n",
        "        'total_questions': len(predictions),\n",
        "        'class_accuracies': class_accuracies,\n",
        "        'confusion_matrix': cm,\n",
        "        'pred_distribution': pred_distribution,\n",
        "        'true_distribution': true_distribution\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Test with a few examples first\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING LLAMA 3.1 WITH SAMPLE QUESTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test with first 3 questions\n",
        "test_df = df.head(3)\n",
        "for idx, row in test_df.iterrows():\n",
        "    print(f\"\\nTest {idx+1}:\")\n",
        "    print(f\"Domain: {row['domain']}\")\n",
        "    print(f\"Sentence: {row['sentence']}\")\n",
        "    print(f\"Options:\")\n",
        "    print(f\"  A. {row['option_A']}\")\n",
        "    print(f\"  B. {row['option_B']}\")\n",
        "    print(f\"  C. {row['option_C']}\")\n",
        "    print(f\"  D. {row['option_D']}\")\n",
        "    print(f\"True Answer: {row['true_label']}\")\n",
        "\n",
        "    result, raw_response = predict_multiple_choice(row)\n",
        "    print(f\"Model Prediction: {result}\")\n",
        "    print(f\"Raw Response: '{raw_response}'\")\n",
        "    print(f\"Correct: {'✓' if result == row['true_label'] else '✗'}\")\n",
        "\n",
        "# Evaluate on a sample first (optional for testing)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING DATASET SAMPLE WITH LLAMA 3.1\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample_predictions, sample_true_labels, sample_raw_responses, sample_df_eval = evaluate_dataset(df, sample_size=20)\n",
        "\n",
        "# Save sample raw predictions\n",
        "sample_raw_predictions_df = pd.DataFrame({\n",
        "    'id': sample_df_eval['id'],\n",
        "    'domain': sample_df_eval['domain'],\n",
        "    'sentence': sample_df_eval['sentence'],\n",
        "    'true_label': sample_true_labels,\n",
        "    'predicted_label': sample_predictions,\n",
        "    'raw_model_response': sample_raw_responses,\n",
        "    'correct': [p == t for p, t in zip(sample_predictions, sample_true_labels)],\n",
        "    'model': 'Llama-3.1-8B',\n",
        "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "})\n",
        "\n",
        "sample_raw_predictions_df.to_csv('llama_raw_model_predictions_sample_mcq.csv', index=False)\n",
        "print(\"\\nSample predictions saved to 'llama_raw_model_predictions_sample_mcq.csv'\")\n",
        "\n",
        "# Calculate metrics for sample\n",
        "sample_metrics = calculate_comprehensive_metrics(sample_predictions, sample_true_labels)\n",
        "print(f\"\\nSample evaluation completed with {sample_metrics['accuracy']*100:.2f}% accuracy!\")\n",
        "\n",
        "# Run on FULL DATASET\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING FULL DATASET WITH LLAMA 3.1 8B\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "full_predictions, full_true_labels, full_raw_responses, df_eval = evaluate_dataset(df)\n",
        "\n",
        "# Save full raw predictions\n",
        "full_raw_predictions_df = pd.DataFrame({\n",
        "    'id': df_eval['id'],\n",
        "    'domain': df_eval['domain'],\n",
        "    'sentence': df_eval['sentence'],\n",
        "    'option_A': df_eval['option_A'],\n",
        "    'option_B': df_eval['option_B'],\n",
        "    'option_C': df_eval['option_C'],\n",
        "    'option_D': df_eval['option_D'],\n",
        "    'true_label': full_true_labels,\n",
        "    'predicted_label': full_predictions,\n",
        "    'raw_model_response': full_raw_responses,\n",
        "    'correct': [p == t for p, t in zip(full_predictions, full_true_labels)],\n",
        "    'model': 'Llama-3.1-8B',\n",
        "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "})\n",
        "\n",
        "full_raw_predictions_df.to_csv('llama_mcq_predictions_full.csv', index=False)\n",
        "print(\"\\nFull predictions saved to 'llama_mcq_predictions_full.csv'\")\n",
        "\n",
        "# Calculate comprehensive metrics for full dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FULL DATASET METRICS\")\n",
        "print(\"=\"*60)\n",
        "full_metrics = calculate_comprehensive_metrics(full_predictions, full_true_labels)\n",
        "\n",
        "# Analyze errors by domain\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DOMAIN-WISE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "domain_results = {}\n",
        "for domain in df_eval['domain'].unique():\n",
        "    domain_mask = df_eval['domain'] == domain\n",
        "    domain_preds = [p for p, m in zip(full_predictions, domain_mask) if m]\n",
        "    domain_true = [t for t, m in zip(full_true_labels, domain_mask) if m]\n",
        "\n",
        "    if len(domain_preds) > 0:\n",
        "        domain_acc = sum(1 for p, t in zip(domain_preds, domain_true) if p == t) / len(domain_preds)\n",
        "        domain_results[domain] = {\n",
        "            'accuracy': domain_acc,\n",
        "            'total': len(domain_preds),\n",
        "            'correct': sum(1 for p, t in zip(domain_preds, domain_true) if p == t)\n",
        "        }\n",
        "        print(f\"{domain}: {domain_acc:.4f} ({domain_results[domain]['correct']}/{domain_results[domain]['total']})\")\n",
        "\n",
        "# Save domain analysis\n",
        "domain_analysis_df = pd.DataFrame.from_dict(domain_results, orient='index')\n",
        "domain_analysis_df.to_csv('llama_domain_wise_analysis.csv')\n",
        "\n",
        "# Analyze misclassified examples\n",
        "misclassified = full_raw_predictions_df[full_raw_predictions_df['correct'] == False]\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(f\"=\"*60)\n",
        "print(f\"Total misclassifications: {len(misclassified)}\")\n",
        "print(f\"Misclassification rate: {len(misclassified)/len(full_raw_predictions_df)*100:.2f}%\")\n",
        "\n",
        "# Save misclassified examples\n",
        "misclassified.to_csv('llama_misclassified_examples_mcq.csv', index=False)\n",
        "\n",
        "# Show sample of misclassified questions\n",
        "if len(misclassified) > 0:\n",
        "    print(\"\\nSample of misclassified questions (showing first 5):\")\n",
        "    sample_errors = misclassified.head(5)\n",
        "    for idx, row in sample_errors.iterrows():\n",
        "        print(f\"\\n--- Error {idx+1} ---\")\n",
        "        print(f\"Domain: {row['domain']}\")\n",
        "        print(f\"Sentence: {row['sentence'][:100]}...\")\n",
        "        print(f\"True Label: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
        "        print(f\"Raw Response: '{row['raw_model_response']}'\")\n",
        "\n",
        "# Create distribution visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "labels = ['A', 'B', 'C', 'D']\n",
        "true_counts = [full_metrics['true_distribution'][l] for l in labels]\n",
        "pred_counts = [full_metrics['pred_distribution'][l] for l in labels]\n",
        "\n",
        "# True labels\n",
        "ax1.bar(labels, true_counts, color='blue', alpha=0.7)\n",
        "ax1.set_title('True Label Distribution')\n",
        "ax1.set_xlabel('Option')\n",
        "ax1.set_ylabel('Count')\n",
        "\n",
        "# Predicted labels\n",
        "ax2.bar(labels, pred_counts, color='green', alpha=0.7)\n",
        "ax2.set_title('Predicted Label Distribution')\n",
        "ax2.set_xlabel('Option')\n",
        "ax2.set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llama_label_distribution_mcq.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save metrics summary\n",
        "metrics_summary = pd.DataFrame([{\n",
        "    'Metric': 'Accuracy',\n",
        "    'Value': full_metrics['accuracy']\n",
        "}, {\n",
        "    'Metric': 'Correct Predictions',\n",
        "    'Value': full_metrics['correct_predictions']\n",
        "}, {\n",
        "    'Metric': 'Total Questions',\n",
        "    'Value': full_metrics['total_questions']\n",
        "}])\n",
        "\n",
        "# Add per-class accuracies\n",
        "for label in ['A', 'B', 'C', 'D']:\n",
        "    metrics_summary = pd.concat([metrics_summary, pd.DataFrame([{\n",
        "        'Metric': f'Class {label} Accuracy',\n",
        "        'Value': full_metrics['class_accuracies'][label]\n",
        "    }])], ignore_index=True)\n",
        "\n",
        "metrics_summary.to_csv('llama_evaluation_metrics_summary_mcq.csv', index=False)\n",
        "\n",
        "# Function to evaluate individual questions\n",
        "def evaluate_single_question(sentence, option_A, option_B, option_C, option_D):\n",
        "    \"\"\"\n",
        "    Evaluate a single multiple choice question\n",
        "    \"\"\"\n",
        "    row = pd.Series({\n",
        "        'sentence': sentence,\n",
        "        'option_A': option_A,\n",
        "        'option_B': option_B,\n",
        "        'option_C': option_C,\n",
        "        'option_D': option_D\n",
        "    })\n",
        "\n",
        "    result, raw_response = predict_multiple_choice(row)\n",
        "\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Options:\")\n",
        "    print(f\"  A. {option_A}\")\n",
        "    print(f\"  B. {option_B}\")\n",
        "    print(f\"  C. {option_C}\")\n",
        "    print(f\"  D. {option_D}\")\n",
        "    print(f\"Model Prediction: {result}\")\n",
        "    print(f\"Raw Response: '{raw_response}'\")\n",
        "\n",
        "    return result, raw_response\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: Llama-3.1-8B\")\n",
        "print(f\"Total Questions Evaluated: {len(df)}\")\n",
        "print(f\"Overall Accuracy: {full_metrics['accuracy']*100:.2f}%\")\n",
        "print(f\"Correct Predictions: {full_metrics['correct_predictions']}/{full_metrics['total_questions']}\")\n",
        "\n",
        "# Create final summary report\n",
        "summary_report = {\n",
        "    'Model': 'Llama-3.1-8B',\n",
        "    'Dataset': 'hard_120_samples.csv',\n",
        "    'Total Questions': full_metrics['total_questions'],\n",
        "    'Correct Predictions': full_metrics['correct_predictions'],\n",
        "    'Overall Accuracy': full_metrics['accuracy'],\n",
        "    'Timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "\n",
        "# Add per-class accuracies to summary\n",
        "for label in ['A', 'B', 'C', 'D']:\n",
        "    summary_report[f'Class_{label}_Accuracy'] = full_metrics['class_accuracies'][label]\n",
        "\n",
        "summary_df = pd.DataFrame([summary_report])\n",
        "summary_df.to_csv('llama_evaluation_summary_mcq.csv', index=False)\n",
        "\n",
        "print(\"\\nResults saved to:\")\n",
        "print(\"  - 'llama_mcq_predictions_full.csv' (full predictions with details)\")\n",
        "print(\"  - 'llama_misclassified_examples_mcq.csv' (incorrect predictions)\")\n",
        "print(\"  - 'llama_domain_wise_analysis.csv' (accuracy by domain)\")\n",
        "print(\"  - 'llama_evaluation_metrics_summary_mcq.csv' (summary metrics)\")\n",
        "print(\"  - 'llama_confusion_matrix_mcq.png' (confusion matrix visualization)\")\n",
        "print(\"  - 'llama_label_distribution_mcq.png' (label distribution visualization)\")\n",
        "print(\"  - 'llama_raw_model_predictions_sample_mcq.csv' (sample predictions)\")\n",
        "\n",
        "print(\"\\nUse evaluate_single_question() to test individual questions.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pt6MTRw3NcTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}